10/22/2025 21:43:25 - INFO - __main__ - ***** Training arguments *****
10/22/2025 21:43:25 - INFO - __main__ - Namespace(config='configs/ddpm.yaml', data_dir='/ocean/projects/cis250019p/jye9/11685-project/data', is_cifar_10=True, image_size=32, batch_size=32, num_workers=4, num_classes=10, run_name='exp-7-ddpm', output_dir='/ocean/projects/cis250019p/jye9/11685-project/experiments', num_epochs=300, learning_rate=0.0005, weight_decay=0, grad_clip=1.0, seed=42, mixed_precision='none', use_val=True, num_train_timesteps=500, num_inference_steps=500, beta_start=0.0001, beta_end=0.1, beta_schedule='cosine', variance_type='fixed_small', prediction_type='epsilon', clip_sample=True, clip_sample_range=1.0, optimizer_type='AdamW', scheduler_type='ConsineAnnealingLR', unet_in_size=32, unet_in_ch=3, unet_ch=64, unet_ch_mult=[1, 2, 2, 4], unet_attn=[2, 3], unet_num_res_blocks=1, unet_dropout=0.2, latent_ddpm=False, pretrained_vae='/ocean/projects/cis250019p/jye9/11685-project/pretrained/model.ckpt', use_cfg=False, cfg_guidance_scale=2.0, use_ddim=False, ckpt=None, device='cuda', rank=0, world_size=1, predictor_type='epsilon', distributed=False, local_rank=0, total_batch_size=32, max_train_steps=468600)
10/22/2025 21:43:25 - INFO - __main__ - ***** Running training *****
10/22/2025 21:43:25 - INFO - __main__ -   Num examples = 50000
10/22/2025 21:43:25 - INFO - __main__ -   Num Epochs = 300
10/22/2025 21:43:25 - INFO - __main__ -   Instantaneous batch size per device = 32
10/22/2025 21:43:25 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
10/22/2025 21:43:25 - INFO - __main__ -   Total optimization steps per epoch 1562
10/22/2025 21:43:25 - INFO - __main__ -   Total optimization steps = 468600
  0%|                                                                                                                                 | 0/468600 [00:00<?, ?it/s]10/22/2025 21:43:25 - INFO - __main__ - Epoch 1/300
