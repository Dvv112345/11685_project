10/22/2025 23:50:53 - INFO - __main__ - ***** Training arguments *****
10/22/2025 23:50:53 - INFO - __main__ - Namespace(config='configs/ddpm.yaml', data_dir='/ocean/projects/cis250019p/jye9/11685-project/data', is_cifar_10=True, image_size=32, batch_size=64, num_workers=4, num_classes=10, run_name='exp-1-ddpm', output_dir='/ocean/projects/cis250019p/jye9/11685-project/experiments', num_epochs=300, learning_rate=0.0001, weight_decay=0.02, grad_clip=1.0, seed=42, mixed_precision='none', use_val=True, num_train_timesteps=500, num_inference_steps=500, beta_start=0.0001, beta_end=0.01, beta_schedule='linear', variance_type='fixed_small', prediction_type='epsilon', clip_sample=True, clip_sample_range=1.0, optimizer_type='AdamW', scheduler_type='ConsineAnnealingLR', unet_in_size=32, unet_in_ch=3, unet_ch=64, unet_ch_mult=[1, 2, 2, 4], unet_attn=[2, 3], unet_num_res_blocks=2, unet_dropout=0.2, latent_ddpm=False, pretrained_vae='/ocean/projects/cis250019p/jye9/11685-project/pretrained/model.ckpt', use_cfg=False, cfg_guidance_scale=2, use_ddim=False, ckpt=None, device='cuda', rank=0, world_size=1, predictor_type='epsilon', distributed=False, local_rank=0, total_batch_size=64, max_train_steps=234300)
10/22/2025 23:50:53 - INFO - __main__ - ***** Running training *****
10/22/2025 23:50:53 - INFO - __main__ -   Num examples = 50000
10/22/2025 23:50:53 - INFO - __main__ -   Num Epochs = 300
10/22/2025 23:50:53 - INFO - __main__ -   Instantaneous batch size per device = 64
10/22/2025 23:50:53 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64
10/22/2025 23:50:53 - INFO - __main__ -   Total optimization steps per epoch 781
10/22/2025 23:50:53 - INFO - __main__ -   Total optimization steps = 234300
  0%|                                                                                                                                 | 0/234300 [00:00<?, ?it/s]10/22/2025 23:50:53 - INFO - __main__ - Epoch 1/300
  0%|▍                                                                                                                    | 781/234300 [01:07<5:29:59, 11.79it/s]/ocean/projects/cis250019p/jye9/conda/envs/11685/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/ocean/projects/cis250019p/jye9/conda/envs/11685/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:1117: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)
  * (1 + math.cos(math.pi * self.last_epoch / self.T_max))
10/22/2025 23:52:01 - INFO - __main__ - Epoch 1/300, Step 780/781, Loss 0.051786623895168304 (0.2537704038024712)
10/22/2025 23:52:03 - INFO - __main__ - Epoch 1/300 Validation Loss: 0.073540
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:07<00:00, 70.07it/s]
  0%|▍                                                                                                                    | 781/234300 [01:20<5:29:59, 11.79it/s]10/22/2025 23:52:13 - INFO - numexpr.utils - Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
10/22/2025 23:52:13 - INFO - numexpr.utils - NumExpr defaulting to 16 threads.
Checkpoint saved at /ocean/projects/cis250019p/jye9/11685-project/experiments/exp-1-ddpm/checkpoints/checkpoint_epoch_0.pth
10/22/2025 23:52:16 - INFO - __main__ - Epoch 2/300
  0%|▌                                                                                                                   | 1128/234300 [01:52<5:31:16, 11.73it/s]
